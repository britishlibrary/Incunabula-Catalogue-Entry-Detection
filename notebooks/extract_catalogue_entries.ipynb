{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5354fc4d",
   "metadata": {},
   "source": [
    "# Extracting zipped parsed page xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f8a6f",
   "metadata": {},
   "source": [
    "This nb originally extracted the zip files containing the results of Isaac's parsing that were uploaded to the project [github](https://github.com/Southampton-Digital-Humanities/2023_Catalogue-Heading-Detection). Once we took over maintaining the code we've switched to processing the data locally so the zipping is obsolete. The majority of the code is now about parsing the xmls exported from Transkribus using updated versions of the code that Isaac wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cabb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from src.data import xml_extraction as xmle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68affd28",
   "metadata": {},
   "source": [
    "### Extract zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1b9da",
   "metadata": {},
   "source": [
    "For when we were extracting zips of Isaac's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_data_path = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\"\n",
    "\n",
    "zips = glob.glob(\"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\\\\*.zip\")\n",
    "\n",
    "# [shutil.unpack_archive(x, x[:-4], format=\"zip\") for x in zips]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6aa17",
   "metadata": {},
   "source": [
    "### Rename download jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\export_job_rename_dict.txt\") as f:  # .txt was a result of massaging the pairings out of Rossitza's .xlsx\n",
    "    rename_lines = f.readlines()\n",
    "    rename_split = [x.split(\":\") for x in rename_lines]\n",
    "    rename_dict = {x[0]:x[1].strip(\"\\n\") for x in rename_split}\n",
    "    \n",
    "rename_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in done_zips:\n",
    "#     renamed = os.path.join(os.path.dirname(folder), export_dict[os.path.basename(folder)])\n",
    "#     os.rename(folder, renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834013f3-ac8a-48b6-9048-1d73d04d977b",
   "metadata": {},
   "source": [
    "## Extracting entries from xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fee7625-4cba-4082-97f3-0a5b6fa03905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 xmls found\n",
      "2 col (109): ..\\data\\raw\\BMC_3_2\\1255673\n",
      "4 col (088): ..\\data\\raw\\BMC_3_4\\1268122\n"
     ]
    }
   ],
   "source": [
    "# currently just working w vol 3, i = 3\n",
    "i = 3\n",
    "two_col_loc = f\"..\\\\data\\\\raw\\\\BMC_{i}_2\\\\*\\\\*.pxml\"\n",
    "four_col_loc = f\"..\\\\data\\\\raw\\\\BMC_{i}_4\\\\*\\\\*.pxml\"\n",
    "xmls_2, xmls_4 = xmle.gen_xml_paths(two_col_loc), xmle.gen_xml_paths(four_col_loc)\n",
    "print(f\"{len(xmls_2) + len(xmls_4)} xmls found\\n\"\n",
    "      f\"2 col ({len(xmls_2):03}): {os.path.dirname(xmls_2[0])}\\n4 col ({len(xmls_4):03}): {os.path.dirname(xmls_4[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eda8434-3e63-4c76-8c5f-b5942c43dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 197/197 [00:09<00:00, 19.86it/s]\n"
     ]
    }
   ],
   "source": [
    "vol_xml_trees = xmle.gen_xml_trees(xmls_2 + xmls_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c088b57a-183b-4dfa-84e4-b2ee902f7a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}Metadata {}\n",
      "{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}Page {'imageFilename': '51195634.jpg', 'imageWidth': '4047', 'imageHeight': '5785'}\n"
     ]
    }
   ],
   "source": [
    "for child in vol_xml_trees[\"J_2704_aa_30_3_0049_4\"]:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a45adc-3aed-4a29-b156-d8c91f5478ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_regions = [x for x in vol_xml_trees[\"J_2704_aa_30_3_0049_4\"][1] if len(x) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef1525-fe4e-43ad-887b-c54feff73961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting catalogue entries from xmls\")\n",
    "lines, xml_track_df = xmle.extract_lines_for_vol(vol_xml_trees)\n",
    "title_shelfmarks, title_indices, ordered_lines = xmle.find_headings(lines)\n",
    "entry_df = xmle.extract_catalogue_entries(ordered_lines, title_indices, title_shelfmarks, xml_track_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c95fe",
   "metadata": {},
   "source": [
    "### Combined volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired globs\n",
    "combined_volumes = []\n",
    "for i in range(1):\n",
    "    network_loc = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\"\n",
    "    suffix = \"column pages Transkribus export\"\n",
    "    combined_volumes.append([os.path.join(network_loc, f\"BMC_{i+1} 2 {suffix}\"), os.path.join(network_loc, f\"BMC_{i+1} 4 {suffix}\")])    \n",
    "    \n",
    "#     When running locally\n",
    "#     combined_volumes.append([f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_2', f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a929a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d0c37",
   "metadata": {},
   "source": [
    "#### Split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(combined_volumes):\n",
    "    print(f\"Combining  {v[0]}\")\n",
    "    txts = glob.glob(os.path.join(v[0], \"splittextfiles\\\\*.txt\")) + glob.glob(os.path.join(v[1], \"splittextfiles\\\\*.txt\"))\n",
    "    single_vol_dir = os.path.join(os.path.dirname(v[0]), f\"BMC_{i+1}\")\n",
    "    if not os.path.exists(single_vol_dir):\n",
    "        os.mkdir(single_vol_dir)\n",
    "    split_fname = os.path.join(single_vol_dir, \"combinedsplittext_single_line.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\", encoding=\"utf-8-sig\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t, encoding=\"utf-8-sig\") as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x.strip(\"\\n\") for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \" \".join(english_only_entry)\n",
    "                split_out.write(split_str + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:incu]",
   "language": "python",
   "name": "conda-env-incu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
