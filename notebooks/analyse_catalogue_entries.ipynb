{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531c846c",
   "metadata": {},
   "source": [
    "# Extracted Catalogue Entry Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf50aac",
   "metadata": {},
   "source": [
    "Analyse catalogue entries extracted by main.py or extract_catalogue_entries.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from cycler import cycler\n",
    "import src.data.xml_extraction as xmle\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129537e-0b30-40af-8b39-1c6498ef5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_word_coords(s):\n",
    "    line_re = re.compile(r\"\\[\\[.*?\\]\\]\")\n",
    "    word_re = re.compile(r\"\\[.*?\\]\")\n",
    "    num_re = re.compile(r\"[\\d]*\")\n",
    "    raw_lines = line_re.findall(s[1:-1])\n",
    "    split_word_lines = [word_re.findall(l[1:-1]) for l in raw_lines]\n",
    "    lines = []\n",
    "    for sw_line in split_word_lines:\n",
    "        line = [[int(x) for x in num_re.findall(word) if x] for word in sw_line]\n",
    "        lines.append(line) \n",
    "    return lines\n",
    "\n",
    "def reconstruct_en_entry(s):\n",
    "    return \" \".join(s[2:-2].split(\"', '\"))\n",
    "\n",
    "def reconstruct_xmls(s):\n",
    "    return s[1:-1].replace(\" \", \"\").replace(\"'\", \"\").split(\",\")\n",
    "        \n",
    "def reconstruct_xml_start_line(s):\n",
    "    return [int(loc) for loc in s[1:-1].replace(\" \", \"\").split(\",\")]\n",
    "\n",
    "converters={\"word_locations\":reconstruct_word_coords, \"en_only\":reconstruct_en_entry, \"xmls\": reconstruct_xmls, \"xml_start_line\": reconstruct_xml_start_line}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75b65b-23c7-4f30-bfff-98a714e7545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using while only working w vol 3\n",
    "\n",
    "# csv_path = r\"..\\data\\processed\\BMC_[0-9]*\\catalogue_entries*.csv\"\n",
    "# entry_csv_paths = glob.glob(csv_path)\n",
    "\n",
    "# entry_csvs = {p.split(\"\\\\\")[-3]: pd.read_csv(p, converters={\"entry\": lambda x: x[2:-2].split(\"\\', \\'\")}) for p in entry_csv_paths}\n",
    "\n",
    "# for vol, df in entry_csvs.items():\n",
    "#     df[\"vol\"] = int(vol.split(\"_\")[-1])\n",
    "\n",
    "# entry_csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry_no_caps_df = pd.concat(list(entry_csvs.values())).rename_axis(index=\"volume_entry_num\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry_df = pd.concat(list(entry_csvs.values())).rename_axis(index=\"volume_entry_num\").reset_index().sort_values(by=[\"vol\", \"volume_entry_num\"])\n",
    "entry_df = pd.read_csv(\"..\\\\data\\\\processed\\\\BMC_3\\\\catalogue_entries_leading_caps.csv\")\n",
    "entry_sm_df = pd.read_csv(\"..\\\\data\\\\processed\\\\BMC_3\\\\catalogue_entries_bought_in.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc6343-f8ef-4537-a7f5-29a408f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd86d9-9186-407d-807c-ae45cc36c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_sm_df.loc[1, \"entry_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35393a-442c-47fa-9455-b8af8f0212e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_sm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2b9db-a69f-44ab-be09-e4d7fb15acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the xml ids present on tkb - if the code didn't find a shelfmark in an xml it won't be represented in matched_records\n",
    "# use the complete list to convert our incomplete list into page numbers on tkb\n",
    "with open(\"..\\\\data\\\\interim\\\\all_xml_ids.txt\", \"r\") as f:\n",
    "    xml_ids = [x.strip(\"\\n\") for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168db31-42c0-4b7e-9cf2-e5801e567819",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xml_df = pd.DataFrame({\"xml\":xml_ids})\n",
    "\n",
    "all_xml_df[\"vol\"] = all_xml_df[\"xml\"].str.split(\"_\").apply(lambda x: int(x[4]))\n",
    "all_xml_df[\"raw_tkb_page\"] = all_xml_df[\"xml\"].str.split(\"_\").apply(lambda x: int(x[5]))\n",
    "all_xml_df[\"num_cols\"] = all_xml_df[\"xml\"].str.split(\"_\").apply(lambda x: int(x[6]))\n",
    "all_xml_df.sort_values(by=[\"vol\", \"num_cols\", \"raw_tkb_page\"], inplace=True)\n",
    "all_xml_df[\"tkb_page\"] = all_xml_df.groupby(by=[\"vol\", \"num_cols\"])[\"raw_tkb_page\"].transform(lambda x: (x.diff() > 0).cumsum() + 1)\n",
    "all_xml_df = all_xml_df.set_index(\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891093d2-7ddb-4ab8-ba6e-51b05f6f2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_tkb_df = entry_df.set_index(\"xml\").merge(right=all_xml_df.drop(columns=\"raw_tkb_page\"), how=\"left\", on=\"xml\").sort_values([\"vol\", \"num_cols\", \"vol_entry_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1fff5-f61b-420b-b5d2-a1d6e0cb49c1",
   "metadata": {},
   "source": [
    "### Trial regexes on all BLL shelfmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29318cb0-3182-4402-a912-c8a67d8cc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bll01_index_df = pd.read_csv(\"..\\\\data\\\\processed\\\\bll01_index.csv\", encoding='latin-1', dtype={\"c_sm\": bool, \"ig_sm\": bool, \"uncaptured_sm\": bool})\n",
    "bll01_index_df.rename(columns={'British Library shelfmark (852 $j)': \"bll01_shelfmark\", 'Record IDs (001)': \"record_id\"}, inplace=True)\n",
    "# merge_df = entry_tkb_df.merge(right=bll01_index_df, how=\"left\", left_on=\"shelfmark\", right_on=\"bll01_shelfmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbffb67b-ea6a-459f-8eaa-6170f2fa17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_re = re.compile(r\"\"\"\n",
    " (?<![A-Za-z0-9\\n\\-\\u201C.])   # Ensure no writing precedes re\n",
    " (?<=[( ])                     # Preceded by space or ( \n",
    " C                             # C of a King Charles lib sm\n",
    " ([.,][ ]?[a-z0-9-*]+)+        # 1+ repeats of [\\.,] ?[a-z0-9-]+ i.e. the characters in the sm\n",
    " ([. ]*[(][0-9. ]*[)])?        # allow followed by a bracketed sets of numbers\n",
    " (?=[., )]|\\Z)                 # lookahead for [\\.,][ )] or end of string\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "c_re = re.compile(r\"\"\"\n",
    " (?<![A-Za-z0-9\\n\\-\\u201C.])   # Ensure no writing precedes re\n",
    " (?<=[( ])                     # Preceded by space or ( \n",
    " C                             # C of a King Charles lib sm\n",
    " \\.[ ]?[0-9]+                  # stop, optional space, 1+ number\n",
    " \\.[ ]?[a-z]+                  # stop, optional space, 1+ letter\n",
    " ([.,][ ]?[0-9-*]+)+           # 1+ (stop/comma, optional space, 1+ number)  \n",
    " ([. ]*[(][0-9. ]+[)])?        # allow followed by a bracketed sets of numbers\n",
    " (?=[ )]|\\Z)                   # lookahead for [\\.,][ )] or end of string\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "ig_re = re.compile(r\"\"\"\n",
    " (?<![A-Za-z0-9\\n\\-\\u201C.])   # Ensure no writing precedes re, effectively only allow a space\n",
    " (I[ABC]|G)                    # Start chars for procter number of Grenville sm\n",
    " ([.,][ ]?[a-z0-9-/A]+)+       # 1+ (stop/comma, optional space, 1+ alphanumeric or dash/slash)\n",
    " \\**                           # allow trailing *\n",
    " ([. ]*[(][0-9-., ]+[)])?        # allow followed by a bracketed sets of numbers\n",
    " (?=[.,) ]|\\Z)                   # lookahead for [\\.,][ )] or end of string\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d4e0c-e9cf-49f6-acce-4f490a7bea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_re.search('G. 7726. (1. ) ; G. 7726. (2. )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e78ac-2051-459b-b373-761fed8a1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bll01_index_df[\"c_re\"] = bll01_index_df[\"bll01_shelfmark\"].apply(lambda x: c_re.search(\"(\" + x))\n",
    "bll01_index_df[\"ig_re\"] = bll01_index_df[\"bll01_shelfmark\"].apply(lambda x: ig_re.search(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6c60c-06d6-4ef0-9249-323011f32826",
   "metadata": {},
   "outputs": [],
   "source": [
    "bll01_index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fa7eb-2f25-4fe3-9036-908dca05aa1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bll01_index_df[bll01_index_df[\"c_sm\"]].iloc[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d78c2a-b0a3-41aa-bff9-5e0cfcbe21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bll01_index_df[~bll01_index_df[\"c_re\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ea909-2be1-4623-adf6-7f225d92deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(bll01_index_df[\"ig_re\"].dropna().apply(lambda x:x.group())) ^ set(bll01_index_df[\"bll01_shelfmark\"][bll01_index_df[\"ig_sm\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbbb41-ac4e-4123-8ab1-0872256af8fd",
   "metadata": {},
   "source": [
    "Styles of unmatched shelfmark\n",
    "- \\[number]. \\[letter]*. \\[number]\n",
    "- \\[number]/\\[number]\n",
    "- Acc|ad|\\[name]\\[. ] \\[number]\\[. ]\\[number]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc431af-95a5-40f2-9c0a-791293d189b9",
   "metadata": {},
   "source": [
    "### Combined and split text for Rossitza's AntConc work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54daef-e71d-49cf-b278-2a1f88116205",
   "metadata": {},
   "source": [
    "#### Remove non-english entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60872562-0150-43bb-93a4-8beec4a9e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_en(s):\n",
    "    try:\n",
    "        return detect(s) == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def get_english_sections(entry):\n",
    "    title = entry[0:1]\n",
    "    entry = pd.Series(entry)\n",
    "    langs = entry.apply(lambda x: detect_en(x))\n",
    "    english_sections = langs.rolling(window=2, closed='both', center=False).mean().bfill() > 0.6\n",
    "    english_only = entry[english_sections].to_list()\n",
    "    if not english_only:\n",
    "        return [\"\"]\n",
    "    if english_only[0] != title:\n",
    "        english_only = title + english_only\n",
    "    return english_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048107be-2902-47f8-8ea8-5bb76c60e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    entry_df = pd.read_csv(f\"..\\\\data\\\\processed\\\\BMC_{i}\\\\catalogue_entries.csv\", converters={\"word_locations\":reconstruct_word_coords})\n",
    "    entry_df[\"entry\"] = entry_df[\"entry_text\"].str.split(\"\\n\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Vol {i}\")\n",
    "        entry_df[\"en_only\"] = entry_df[\"entry\"].progress_apply(lambda x: get_english_sections(x))\n",
    "        entry_df.to_csv(f\"..\\\\data\\\\processed\\\\BMC_{i}\\\\catalogue_entries_en_only.csv\")\n",
    "    except IndexError:\n",
    "        print(f\"{i} failed\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6651f-7dab-47fb-b4fb-4844f033b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    entry_df = pd.read_csv(f\"..\\\\data\\\\processed\\\\BMC_{i}\\\\catalogue_entries_en_only.csv\", converters={\"word_locations\":reconstruct_word_coords, \"en_only\":reconstruct_en_entry})\n",
    "\n",
    "    try:\n",
    "        print(f\"Vol {i}\")\n",
    "        with open(f\"..\\\\data\\\\processed\\\\BMC_{i}\\\\BMC_{i}_split_text_single_line_v1.2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            trailing_linebreak_only = entry_df[\"en_only\"] + \"\\n\"\n",
    "            export_text = trailing_linebreak_only.sum()\n",
    "            f.write(export_text)\n",
    "    except IndexError:\n",
    "        print(f\"{i} failed\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562298b-e83d-4025-abab-c39dfd556229",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = \"..\\\\data\\\\processed\\\\BMC_1_10_combined_split_text_single_line_v1.2.txt\"\n",
    "for i in range(1,10):\n",
    "    print(f\"Vol {i}\")\n",
    "    vol_text = f\"..\\\\data\\\\processed\\\\BMC_{i}\\\\BMC_{i}_split_text_single_line_v1.2.txt\" \n",
    "    with open(vol_text, \"r\", encoding=\"utf-8\") as f, open(combined_text, \"a\", encoding=\"utf-8\") as g:\n",
    "        vol_lines = f.read()\n",
    "        g.write(vol_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffecb4e-8a10-4c4f-8b71-4ffe60576e03",
   "metadata": {},
   "source": [
    "### Work with extracted entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068fbd5-1211-4f9b-8752-88919aa473c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df = pd.read_csv(\"..\\\\data\\\\processed\\\\BMC_1\\\\catalogue_entries_complete_xmls.csv\", converters=converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05ea46-4810-4f0f-9375-75cae88dbbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28d59c-8563-4f0e-9491-0d583417a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word_locs(row):\n",
    "    if len(row[\"xml_start_line\"]) == 1:\n",
    "        return [row[\"word_locations\"]]\n",
    "    else:\n",
    "        return [row[\"word_locations\"][start:end] for start, end in zip([0] + row[\"xml_start_line\"], row[\"xml_start_line\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb237f-4d47-4f6c-bd6a-9cf5902c71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_page_entries_lookup(df):\n",
    "    xml_list = df[\"xmls\"].sum()\n",
    "    word_locs_split = df.apply(split_word_locs, axis=1).sum()\n",
    "\n",
    "    page_entries_df = pd.DataFrame(data={\"xml\":xml_list, \"word_locs\":word_locs_split})\n",
    "    page_entries_df[\"word_locs\"] = page_entries_df[\"word_locs\"].apply(lambda x: [x])\n",
    "\n",
    "    page_entries_lookup = page_entries_df.groupby(by=\"xml\", as_index=False).sum()\n",
    "    pages_non_zeroed = page_entries_lookup[\"xml\"].apply(lambda x: int(x.split(\"_\")[-2]))\n",
    "    page_entries_lookup[\"page\"] = (pages_non_zeroed - (pages_non_zeroed.min() - 1)).values\n",
    "    page_entries_lookup[\"n_entries\"] = page_entries_lookup[\"word_locs\"].apply(len)\n",
    "    return page_entries_lookup.set_index(\"page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd7c9d-0a83-4921-983e-6c442abbdc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_entries_lookup = gen_page_entries_lookup(entry_df)\n",
    "page_entries_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730035ab-49cf-4551-b8c7-6a759bcc3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_h(ims):\n",
    "    # all ims are not exactly same size - differ by maybe 10%\n",
    "    widths = [im.width for im in ims]\n",
    "    cumsum_width = [0] + [sum(widths[:i+1]) for i in range(len(widths))]\n",
    "    total_width = sum([im.width for im in ims])\n",
    "    dst = Image.new('RGBA', (total_width, ims[0].height))\n",
    "    [dst.paste(im, (x_start, 0)) for im, x_start in zip(ims, cumsum_width[:-1])]\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc222c-776f-4bdc-b6ef-0a3bd19ddff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_entry(df_row):\n",
    "    xmls = df_row[\"xmls\"]\n",
    "    start_lines = df_row[\"xml_start_line\"]\n",
    "    vols = [xml.split(\"_\")[-3] for xml in xmls]\n",
    "    cols = [xml[-1] for xml in xmls]\n",
    "    jpgs = [xml[:-2] for xml in xmls]\n",
    "    image_paths = [glob.glob(f\"..\\\\data\\\\raw\\\\BMC_{vol}_{col}\\\\*\\\\{jpg}.jpg\")[0] for vol, col, jpg in zip(vols, cols, jpgs)]\n",
    "\n",
    "    # get an image\n",
    "    word_locs = df_row[\"word_locations\"]\n",
    "    out_images = []\n",
    "    for path, start, cutoff in zip(image_paths, [0] + start_lines, start_lines):\n",
    "        with Image.open(path).convert(\"RGBA\") as base:\n",
    "        \n",
    "            # make a blank image for the colour patches, initialized to transparent\n",
    "            patches = Image.new(\"RGBA\", base.size, (255, 255, 255, 0))\n",
    "            \n",
    "            draw = ImageDraw.Draw(patches)\n",
    "            for line in word_locs[start:cutoff]:\n",
    "                [draw.rectangle(word, fill=(237, 232, 74, 65)) for word in line]\n",
    "        \n",
    "            out = Image.alpha_composite(base, patches)\n",
    "            \n",
    "            width, height = base.width // 6, base.height // 6\n",
    "            out_images.append(out.resize((width, height)))\n",
    "\n",
    "    concat_out = get_concat_h(out_images)\n",
    "    display(concat_out)\n",
    "\n",
    "\n",
    "colours = colormaps[\"Accent\"].colors[:4]\n",
    "colours = [[int(255 * x) for x in list(c)] + [65] for c in colours]\n",
    "pastel_cycler = cycler(color=colours)\n",
    "\n",
    "\n",
    "def display_page(page, page_entry_lookup):\n",
    "    xml = page_entry_lookup.loc[page, \"xml\"]\n",
    "    entries = page_entry_lookup.loc[page, \"word_locs\"]\n",
    "    cc = pastel_cycler()\n",
    "    colours = [c['color'] for c, _ in zip(cc, entries)]\n",
    "    vol = xml.split(\"_\")[-3]\n",
    "    col = xml[-1]\n",
    "    jpg = xml[:-2]\n",
    "    path = glob.glob(f\"..\\\\data\\\\raw\\\\BMC_{vol}_{col}\\\\*\\\\{jpg}.jpg\")[0]\n",
    "\n",
    "    with Image.open(path).convert(\"RGBA\") as base:\n",
    "        \n",
    "        # make a blank image for the colour patches, initialized to transparent\n",
    "        patches = Image.new(\"RGBA\", base.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(patches)\n",
    "        \n",
    "        for word_locs, colour in zip(entries, colours):\n",
    "            for line in word_locs:\n",
    "                [draw.rectangle(word, fill=tuple(colour)) for word in line]\n",
    "        \n",
    "            out = Image.alpha_composite(base, patches)\n",
    "            \n",
    "        width, height = base.width // 8, base.height // 8\n",
    "        resized = out.resize((width, height))\n",
    "        display(resized)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f1868-77b6-4bfa-804f-68c4e3b6c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_4, page_5 = display_page(4, page_entries_lookup), display_page(5, page_entries_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b63e2f-ea05-413e-9da6-d5e66b077f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concat_h([page_4, page_5]).save(\"..\\\\reports\\\\figures\\\\categorised_spread.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae76d5b-0a29-4261-afa7-d3e1c838dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_entry(entry_df.loc[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e023f1",
   "metadata": {},
   "source": [
    "## Entry length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df[\"entry_length\"] = entry_df[\"entry_text\"].transform(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39611208",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = entry_df[\"entry_length\"].rolling(window=100, center=True).mean()\n",
    "mean = entry_df.groupby(by=\"vol\")[\"entry_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076991c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe923d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean.rename_axis(\"Volume\").rename(\"Mean Entry Length\").to_csv(\"..\\\\data\\\\processed\\\\mean_lengths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entrys = entry_df.groupby(by=\"vol\")[\"vol\"].count()\n",
    "n_entrys.loc[0] = 0\n",
    "n_entrys.sort_index(inplace=True)\n",
    "x_locs = n_entrys.cumsum() - n_entrys.cumsum().diff()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(entry_df[\"entry_length\"], lw=1)\n",
    "ax.plot(ma, \"black\", label=\"Moving average\")\n",
    "ax.set_title(\"Catalogue Entry Length For Incunabula Volumes 1-10\", fontsize='x-large')\n",
    "ax.set_xlabel(\"Catalogue Entry Number (across all volumes)\", fontsize='x-large')\n",
    "ax.set_ylabel(\"Entry length (characters)\", fontsize='x-large')\n",
    "ax.tick_params(labelsize='large')\n",
    "ax.vlines(n_entrys.cumsum(), 0, ax.get_ylim()[1], colors=\"black\", linestyles=\"--\")\n",
    "ax.set_xlim(0, len(entry_df))\n",
    "ax.set_ylim(0, entry_df[\"entry_length\"].max() + 100)\n",
    "for i, x in enumerate(x_locs.dropna()[:8]):\n",
    "    ax.text(x, 10600, f\"BMC {i+1}\\n$\\mu$: {mean.loc[i+1]:.0f}\", ha=\"center\")\n",
    "    \n",
    "\n",
    "ax.text(x_locs[9], 10600, f\"BMC {9}\", rotation=\"vertical\", ha=\"center\")\n",
    "ax.text(x_locs[10], 10600, f\"BMC {10}\", rotation=\"vertical\", ha=\"center\")\n",
    "ax.text(x_locs[9], 9100, f\"$\\mu$: {mean.loc[9]:.0f}\", rotation=\"vertical\", ha=\"center\")\n",
    "ax.text(x_locs[10], 9100, f\"$\\mu$: {mean.loc[10]:.0f}\", rotation=\"vertical\", ha=\"center\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6384fe",
   "metadata": {},
   "source": [
    "Vol 5 and 8 were catalogued by the same person, so poss more errors here or actually reflecting cataloguing style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.query(\"entry_length > 6000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"..\\\\reports\\\\figures\\\\entry_length.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1189050",
   "metadata": {},
   "source": [
    "## Another copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_another_copy(row):\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    another_variants = [\n",
    "        'Another cancelled',\n",
    "        'A cancelled',\n",
    "        'Another copy',\n",
    "        'Another edition',\n",
    "        'Another fragment,',\n",
    "        'Another issue'\n",
    "    ]\n",
    "    \n",
    "    match = []\n",
    "    for v in another_variants:\n",
    "        p = re.compile(v)\n",
    "        m = p.finditer(row)\n",
    "        if m:\n",
    "            match += m\n",
    "    \n",
    "    if match:\n",
    "        return match\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to find valid variants of another copy\n",
    "\"\"\"\n",
    "copy_re = re.compile(\"Another \\S*\")\n",
    "anothers = entry_df[\"entry_text\"].apply(lambda x: copy_re.search(x))\n",
    "copy_variants = sorted(list(set(anothers.apply(lambda x: x.group() if x else None).dropna())))\n",
    "copy_variants\n",
    "\n",
    "entry_df[\"match\"] = entry_df[\"entry_text\"].apply(lambda x: copy_re.search(x))\n",
    "entry_df[\"preceding_shelfmark\"] = entry_df.apply(check_for_leading_shelfmark, axis=1)\n",
    "\n",
    "x = 2\n",
    "print(copy_variants[x])\n",
    "entry_df[entry_df[\"entry_text\"].str.contains(copy_variants[x])]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c838c5f",
   "metadata": {},
   "source": [
    "All the matches of the \"Another \\S*\" regex with statement as to whether consists of actual 'Another edition' information.\n",
    "\n",
    "\n",
    "'Another (crown':  Not valid, referring to watermarks in the text  \n",
    "'Another calendar':  Not valid, referring to calendars in the work  \n",
    "'Another calligraphic':  Not valid, referring to calligraphic letters  \n",
    "'Another cancelled':  Valid, has it's own Proctor # and copy specific info. There's also a copy before this that's just \"A cancelled copy\", but there's only one occurence of this.  \n",
    "'A cancelled': Valid, see above entry.  \n",
    "'Another closely':  Not valid, describes another edition that's similar  \n",
    "'Another compartment':  Not valid, part of the information rather than about another copy  \n",
    "'Another copy':  Valid  \n",
    "'Another copy,':  Subset of Another copy  \n",
    "'Another copy.':  Subset of Another copy  \n",
    "'Another cut':  Not valid  \n",
    "'Another edition':  Valid  \n",
    "'Another edition,':  Subset  \n",
    "'Another edition.':  Subset  \n",
    "'Another fragment,':  Valid  \n",
    "'Another full-page':  Not valid  \n",
    "'Another issue':  Valid  \n",
    "'Another issue,':  Subset  \n",
    "'Another issue.':  Subset  \n",
    "'Another metrical':  Not valid  \n",
    "'Another reading.':  Not valid  \n",
    "'Another recension':  Not valid  \n",
    "'Another setting':  Not valid  \n",
    "'Another setting-up':  Subset  \n",
    "'Another version:  Not valid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_variants = [\n",
    "    'Another cancelled',\n",
    "    'A cancelled',\n",
    "    'Another copy',\n",
    "    'Another edition',\n",
    "    'Another fragment,',\n",
    "    'Another issue'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57adeb7",
   "metadata": {},
   "source": [
    "Having a leading shelfmark is highly indicative of an 'Another copy' entry actually being another copy. Of course this relies on the shelfmark detection being accurate. In some cases this isn't so, see analysis below for efforts to improve Issac's shelfmark finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO some of the shelfmarks are absent - instead the Another copy has it's location listed as \"Print room\"\n",
    "# work out what to do with this\n",
    "\n",
    "def check_for_leading_shelfmark(row, match_col, find_valid=True):\n",
    "    shelfmark = False\n",
    "    valid_matches = []\n",
    "    if row[match_col]:\n",
    "        valid = [xmle.find_title_shelfmark(row[\"entry_text\"][match.span()[0]-100: match.span()[1]]) for match in row[match_col]]\n",
    "        if not find_valid:\n",
    "            valid = [not v for v in valid]\n",
    "        valid_matches = [m for m, v in zip(row[match_col], valid) if v] \n",
    "    if valid_matches:\n",
    "        return valid_matches\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_regex = re.compile(\"[A-Z][A-Z](?!I)[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980750c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df[\"other_copies\"] = entry_df[\"entry_text\"].apply(lambda x: extract_another_copy(x))\n",
    "entry_df[\"valid_copies\"] = entry_df.apply(check_for_leading_shelfmark, match_col=\"other_copies\", axis=1)\n",
    "entry_df[\"bad_copies\"] = entry_df.apply(check_for_leading_shelfmark, match_col=\"other_copies\", find_valid=False, axis=1)\n",
    "entry_df[\"leading_caps\"] = entry_df[\"entry_text\"].apply(lambda x: caps_regex.match(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "entry_df.loc[idx, \"entry_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_regex.search(entry_df.loc[idx, \"entry_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7948d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.loc[idx, \"entry_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iiif_vol_manifests = {\n",
    "    1: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100187977347.0x000001/manifest.json\", \n",
    "    2: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186434238.0x000001/manifest.json\", \n",
    "    3: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186508207.0x000001/manifest.json\", \n",
    "    4: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186435804.0x000001/manifest.json\", \n",
    "    5: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186440144.0x000001/manifest.json\", \n",
    "    6: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186441763.0x000001/manifest.json\", \n",
    "    7: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186508670.0x000001/manifest.json\", \n",
    "    8: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100186436306.0x000001/manifest.json\", \n",
    "    9: \"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100187984642.0x000001/manifest.json\", \n",
    "    10:\"http://api.bl.uk/metadata/iiif/ark:/81055/vdc_100187985363.0x000001/manifest.json\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url_stems = []\n",
    "# thumbnail_pattern = re.compile(\"https://api\\.bl\\.uk/image/iiif/ark:/81055/vdc_\\d*\")\n",
    "# for url in iiif_vol_manifests.values():\n",
    "#     manifest = requests.get(url)\n",
    "#     image_url_stems.append(thumbnail_pattern.search(manifest.text).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iiif_image_url_stems = {\n",
    "    1: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188432072.0x',\n",
    "    2: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188433159.0x',\n",
    "    3: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188433874.0x',\n",
    "    4: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188438804.0x',\n",
    "    5: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188433623.0x',\n",
    "    6: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188434221.0x',\n",
    "    7: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188432690.0x',\n",
    "    8: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188432452.0x',\n",
    "    9: 'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188432911.0x',\n",
    "    10:'https://api.bl.uk/image/iiif/ark:/81055/vdc_100188439091.0x'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f547f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_api_options = \"/full/800,/0/default.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92106eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iiif_image_url(iiif_vol_url, page, image_api_options):\n",
    "    hex_page = f'{int(page):06x}'\n",
    "    image_url = f\"{iiif_vol_url}{hex_page.rjust(6, '0')}{image_api_options}\"\n",
    "    return image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fff13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = entry_df.loc[idx, \"vol\"]\n",
    "page = entry_df.loc[idx, \"xml\"].split(\"_\")[-2]\n",
    "image_url = iiif_image_url(iiif_image_url_stems[vol], page, image_api_options)\n",
    "display(Image(image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe62885",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = iiif_image_url(iiif_image_url_stems[vol], int(page) + 1, image_api_options)\n",
    "display(Image(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951effa",
   "metadata": {},
   "source": [
    "### Assess regex variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822bd8a5",
   "metadata": {},
   "source": [
    "Some of the 'another copy' leading shelfmarks aren't being picked up. Improve the original shelfmark detection, particularly C numbers (which are sometimes '1' numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_regex = re.compile(\"[A-Z]{3,}\")\n",
    "# c_num_regex = re.compile(\"[^I]C\\\\.[0-9]\")  # C number title references\n",
    "# c_num_space_regex = re.compile(\"[^I]C\\\\.[ ]?[0-9]\")  # C number title references\n",
    "c_num_regex = re.compile(\"[^A-Za-z0-9\\\\n\\.\\-\\u201C]C\\\\.[ ]?[0-9]\")  # C number title references\n",
    "c_date_regex = re.compile(\"[^I]C\\\\.[ \\t\\r\\f\\v]?1[0-9]{3}[^0-9]\")  # accidental date references\n",
    "one_num_regex = re.compile(\"1\\\\.\\\\s[a-z]\")\n",
    "g_num_regex = re.compile(\"G.[ ]?[0-9]\")\n",
    "i_num_regex = re.compile(r\"[I1][ABC]\\\\.[ ]?[0-9]\")  # I number title references\n",
    "date_regex = re.compile(\"1[45][0-9][0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nums = entry_df[\"entry_text\"].apply(lambda x: c_num_regex.finditer(x)).apply(lambda x: [x for x in x]).apply(lambda x: x if len(x) > 0 else None)\n",
    "c_dates = entry_df[\"entry_text\"].apply(lambda x: c_date_regex.finditer(x)).apply(lambda x: [x for x in x]).apply(lambda x: x if len(x) > 0 else None)\n",
    "\n",
    "entry_df[\"c_nums\"] = c_nums\n",
    "entry_df[\"c_dates\"] = c_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_date_matches(row):\n",
    "    if row[\"c_dates\"] and row[\"c_nums\"]:\n",
    "        for r in row[\"c_dates\"]:\n",
    "            date_span = set(range(*r.span()))\n",
    "            accidental_date = [date_span.intersection(set(range(*x.span()))) for x in row[\"c_nums\"]]\n",
    "            \n",
    "        clean_cnums = [x for x,y in zip(row[\"c_nums\"], accidental_date) if not y]\n",
    "        if clean_cnums:\n",
    "            return clean_cnums\n",
    "        else:\n",
    "            return row[\"c_nums\"]\n",
    "    else:\n",
    "        return row[\"c_nums\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df[\"clean_c_nums\"] = entry_df.apply(exclude_date_matches, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_idx = entry_df[\"clean_cnums\"].dropna().apply(lambda x: [x.span() for x in x]).index.difference(entry_df[\"let_cnums\"].dropna().apply(lambda x: [x.span() for x in x]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(row, match_row):\n",
    "    matches = []\n",
    "    if not row[match_row]:\n",
    "        return None\n",
    "    for match in row[match_row]:\n",
    "        matches.append(row[\"entry_text\"][match.span()[0]:match.span()[1] + 20])\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "def twenty_plus_gen():\n",
    "    global i\n",
    "    i += 10\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21933e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.apply(find_matches, match_row=\"clean_c_nums\", axis=1).dropna().iloc[twenty_plus_gen()-10: twenty_plus_gen()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49a1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 4767\n",
    "entry_df.loc[idx, \"entry_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = entry_df.loc[idx, \"vol\"]\n",
    "page = entry_df.loc[idx, \"xml\"].split(\"_\")[-2]\n",
    "image_url = iiif_image_url(iiif_image_url_stems[vol], page, image_api_options)\n",
    "display(Image(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497d97d",
   "metadata": {},
   "source": [
    "### Outdated image loading from network drive rather than IIIF manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdb85a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol = entry_df.loc[idx, \"vol\"]\n",
    "col = int(entry_df.loc[idx, \"xml\"][-1])\n",
    "jpg = entry_df.loc[idx, \"xml\"][:-2]\n",
    "image_path = glob.glob(\n",
    "    r\"\\\\ad\\collections\\TwoCenturies\\TwoCenturies IV\\Incunabula\"\n",
    "    f\"\\\\BMC_{vol} {col} column pages Transkribus export\"\n",
    "    f\"\\\\*\\\\*\\\\*{jpg}.jpg\"\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1704db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attempts = 0\n",
    "while attempts < 10:\n",
    "    try:\n",
    "        display(Image(filename=image_path))\n",
    "        break\n",
    "    except:\n",
    "        attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    next_jpg = jpg[:-1] + str(int(jpg[-1]) + 1)\n",
    "    next_image_path = glob.glob(\n",
    "        r\"\\\\ad\\collections\\TwoCenturies\\TwoCenturies IV\\Incunabula\"\n",
    "        f\"\\\\BMC_{vol} {col} column pages Transkribus export\"\n",
    "        f\"\\\\*\\\\*\\\\*{next_jpg}.jpg\"\n",
    "    )[0]\n",
    "    \n",
    "except IndexError:\n",
    "    next_jpg = jpg[:-1] + str(int(jpg[-1]) + 1)\n",
    "    if int(col) == 2:\n",
    "        next_col = 4\n",
    "    elif int(col) == 4:\n",
    "        next_col = 2\n",
    "    next_image_path = glob.glob(\n",
    "        r\"\\\\ad\\collections\\TwoCenturies\\TwoCenturies IV\\Incunabula\"\n",
    "        f\"\\\\BMC_{vol} {next_col} column pages Transkribus export\"\n",
    "        f\"\\\\*\\\\*\\\\*{next_jpg}.jpg\"\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = 0\n",
    "while attempts < 10:\n",
    "    try:\n",
    "        display(Image(filename=next_image_path))\n",
    "        break\n",
    "    except:\n",
    "        attempts += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:incu]",
   "language": "python",
   "name": "conda-env-incu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
